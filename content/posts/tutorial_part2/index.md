---
title: "Hands-on CamemBERT, partie 2: Fine-tuner Camembert pour la Classification d'Acte de Dialogue"
date: "2022-07-06T00:00:01Z"
authors: 
- Benjamin Muller
- Nathan Godey
- Roman Castagn√©
categories: 
- nlp
- camembert
- classification
tags:

featured: true

features:
  math: true
---

![png](/img/icon-192.png)

Ce tutoriel a √©t√© con√ßu dans le cadre des journ√©es [Deep Voice de l'IRCAM](https://www.ircam.fr/agenda/deep-voice-paris/detail) par Roman Castagn√©, Nathan Godey et Benjamin Muller.

Une version du tutoriel en anglais en version pdf slide est disponible [ici](/pdf/hands-on-camembert-tutorial-slides-en.pdf)

Suite √† la premi√®re partie qui introduis le fonctionnement de CamemBERT en tant que mod√®le de langue (disponible [ici]({{< relref "posts/tutorial" >}})), nous allons d√©sormais √©tudier comment entrainer et √©valuer (*fine-tuner* pour √™tre pr√©cis!) CamemBERT pour notre t√¢che de classification d'acte de dialogue.


# Dans ce Tutoriel:

Ce tutoriel est une introduction au Natural Language Processing et en particulier au mod√®le de langue de type BERT. 

## Pour cela

Nous allons nous int√©resser √† une t√¢che de classification de s√©quence: **la t√¢che de pr√©diction d'acte de dialogue**. 

Nous allons utiliser le dataset MIAM (introduis [ici](https://aclanthology.org/2021.emnlp-main.656.pdf)) afin d'entra√Æner et d'√©valuer nos mod√®les sur cette t√¢che. 

Nous travaillerons avec la librairie `transformers` de Hugging-Face ü§ó ainsi que la librairie `pytorch-lightning`.

## Pr√©requis

- bases en python 
- bases en machine learning



# Partie 2 : *Finetuning* pour la classification de s√©quences

Dans ce chapitre, nous allons finetuner CamemBERT pour une t√¢che de classification de s√©quences.

L'objectif de cette partie est double :
- Apprendre √† utiliser des modules Python modernes qui permettent d'entra√Æner rapidement les mod√®les de langue (HuggingFace Transformers et Datasets, Pytorch Lightning) ;
- Etudier les performances du mod√®le de fran√ßais CamemBERT notamment en comparaison avec un mod√®le sans pr√©-entra√Ænement pr√©alable (uniquement entra√Æn√© sur la t√¢che de classification) et un mod√®le avec un pr√©-entra√Ænement multilingue.



### Inspection des donn√©es pour le mod√®le

Notre jeu de donn√©es √©tant relativement petit, nous faisons le choix de tokeniser nos donn√©es juste avant de les fournir au mod√®le. La pr√©paration est relativement simple dans notre cas. Nous allons devoir tokeniser les donn√©es, les "padder" (i.e. s'assurer que toutes les s√©quences ont la m√™me longueur) et rajouter des tokens sp√©ciaux. 

Toutes ces op√©rations ont d√©j√† √©t√© faites dans la premi√®re partie du notebook, nous r√©utilisons donc le dataset et la *collate function* `tokenize_batch` afin d'instancier un `DataLoader` qui va fournir les donn√©es au mod√®le en *batch*.

Un batch est un groupe d'exemples donn√©s au mod√®le √† une √©tape d'entra√Ænement. La *loss* et les gradients sont calcul√©s et moyenn√©s pour l'ensemble du batch. Cela permet:
- d'√©viter une trop grande variance des gradients utilis√©s pour la descente de gradients stochastique: en moyennant les gradients, la direction de descente estim√©e √† chaque pas est plus proche de la direction de descente th√©orique;
- de profiter de la parall√©lisation des calculs offerte par les GPUs pour certaines op√©rations. On peut ainsi observer une inf√©rence plus rapide avec de plus grands batchs pour un m√™me nombre d'exemples total.


```python
num_labels = dataset["train"].features["Label"].num_classes
```

Re-regardons quelques exemples de phrases issues du jeu de donn√©es MIAM. On retrouve √† gauche les labels que nous allons devoir pr√©dire.


```python
pd_dataset["validation"][["Dialogue_Act", "Utterance"]].head(40)
```





  <div id="df-07cd8a6c-547c-4135-b820-7564ec16305b">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Dialogue_Act</th>
      <th>Utterance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>greet</td>
      <td>Bravo! Vous avez √©t√© rapides!</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ask</td>
      <td>Qu'est-ce que je peux faire pour vous?</td>
    </tr>
    <tr>
      <th>2</th>
      <td>next_step</td>
      <td>merci</td>
    </tr>
    <tr>
      <th>3</th>
      <td>inform</td>
      <td>Eh bien, il va falloir la fabriquer cette mane...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ask</td>
      <td>Mais sinon, vous avez encore des questions sur...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>staff_enterprise</td>
      <td>j'ai un peu de temps... allez y dites moi en p...</td>
    </tr>
    <tr>
      <th>6</th>
      <td>inform</td>
      <td>la plasturgie, c' est plus une question de log...</td>
    </tr>
    <tr>
      <th>7</th>
      <td>ask</td>
      <td>et vous vouliez savoir quelles √©tudes ils avai...</td>
    </tr>
    <tr>
      <th>8</th>
      <td>yes</td>
      <td>oui oui</td>
    </tr>
    <tr>
      <th>9</th>
      <td>inform</td>
      <td>Il y a des m√©tiers pour tout le monde¬†du BEP √†...</td>
    </tr>
    <tr>
      <th>10</th>
      <td>quit</td>
      <td>Bon, je dois vous laisser. Bon courage pour la...</td>
    </tr>
    <tr>
      <th>11</th>
      <td>greet</td>
      <td>Bravo! Vous avez √©t√© rapides!</td>
    </tr>
    <tr>
      <th>12</th>
      <td>ask</td>
      <td>Qu'est-ce que je peux faire pour vous?</td>
    </tr>
    <tr>
      <th>13</th>
      <td>next_step</td>
      <td>est ce que je peux utiliser la manette mainten...</td>
    </tr>
    <tr>
      <th>14</th>
      <td>inform</td>
      <td>Il ne vous reste plus qu' √† trouver les compos...</td>
    </tr>
    <tr>
      <th>15</th>
      <td>ask</td>
      <td>Mais avant de partir, voulez vous faire un min...</td>
    </tr>
    <tr>
      <th>16</th>
      <td>yes</td>
      <td>oui</td>
    </tr>
    <tr>
      <th>17</th>
      <td>ask</td>
      <td>Que faudra-t-il faire de votre manette une foi...</td>
    </tr>
    <tr>
      <th>18</th>
      <td>todo_irreparable</td>
      <td>la recycler ?</td>
    </tr>
    <tr>
      <th>19</th>
      <td>inform</td>
      <td>tout √† fait, on ram√®ne les √©quipements hors-se...</td>
    </tr>
    <tr>
      <th>20</th>
      <td>quit</td>
      <td>J'aurais aim√© continuer √† r√©pondre √† vos quest...</td>
    </tr>
    <tr>
      <th>21</th>
      <td>greet</td>
      <td>Bonjour !</td>
    </tr>
    <tr>
      <th>22</th>
      <td>ask</td>
      <td>Il faut que tu trouves l' adresse de l' entrep...</td>
    </tr>
    <tr>
      <th>23</th>
      <td>help</td>
      <td>Ok, o√π se trouve cette entreprise?</td>
    </tr>
    <tr>
      <th>24</th>
      <td>inform</td>
      <td>Je ne sais pas . . . mon oncle a dit que c' √™t...</td>
    </tr>
    <tr>
      <th>25</th>
      <td>quit</td>
      <td>Ok, c'est super!</td>
    </tr>
    <tr>
      <th>26</th>
      <td>greet</td>
      <td>Bonjour, je suis Preparateur1 le pr√©parateur m...</td>
    </tr>
    <tr>
      <th>27</th>
      <td>ask</td>
      <td>Qu'est-ce que je peux faire pour vous?</td>
    </tr>
    <tr>
      <th>28</th>
      <td>inform_material_space</td>
      <td>cet endroit m'int√©resse, que pouvez vous me di...</td>
    </tr>
    <tr>
      <th>29</th>
      <td>inform</td>
      <td>Eh bien quand on re√ßoit les granul√©s de mati√®r...</td>
    </tr>
    <tr>
      <th>30</th>
      <td>ask</td>
      <td>vous voulez savoir ce que je fais moi, plus pa...</td>
    </tr>
    <tr>
      <th>31</th>
      <td>yes</td>
      <td>oui, √ßa m'int√©resser !</td>
    </tr>
    <tr>
      <th>32</th>
      <td>inform</td>
      <td>mon r√¥le, c' est de m' assurer que les machine...</td>
    </tr>
    <tr>
      <th>33</th>
      <td>ask</td>
      <td>Voulez vous en savoir plus?</td>
    </tr>
    <tr>
      <th>34</th>
      <td>no</td>
      <td>non, merci, nous devons filer. Mais ce fut pas...</td>
    </tr>
    <tr>
      <th>35</th>
      <td>ask</td>
      <td>attendez, ne partez pas si vite, j'ai un petit...</td>
    </tr>
    <tr>
      <th>36</th>
      <td>yes</td>
      <td>pas de soucis, je vais vous aider</td>
    </tr>
    <tr>
      <th>37</th>
      <td>inform</td>
      <td>ok, c' est parti</td>
    </tr>
    <tr>
      <th>38</th>
      <td>quit</td>
      <td>Merci de votre aide, bonne journ√©e √† vous !</td>
    </tr>
    <tr>
      <th>39</th>
      <td>greet</td>
      <td>D√©j√† de retour avec les plans ?! Bravo les jeu...</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-07cd8a6c-547c-4135-b820-7564ec16305b')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-07cd8a6c-547c-4135-b820-7564ec16305b button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-07cd8a6c-547c-4135-b820-7564ec16305b');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




Il est important, surtout avec de petits jeux de donn√©es, que le mod√®le ne voit pas les donn√©es dans le m√™me ordre. Pour √©viter cela, on utilise l'argument `shuffle` du DataLoader.


```python
train_dataloader = DataLoader(
    dataset["train"], 
    batch_size=16, 
    shuffle=True, 
    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer)
)
val_dataloader = DataLoader(
    dataset["validation"], 
    batch_size=16, 
    shuffle=False, 
    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer)
)
```

Une bonne mani√®re de s'assurer que les donn√©es sur lesquelles le mod√®le va s'entra√Æner sont dans un format correct est de regarder un batch et d√©coder les indices issus du tokenizer. On retrouve ce √† quoi l'on s'attendait : les tokens sp√©ciaux et le padding pour compenser les phrases trop courtes.

On remarque notamment que dans CamemBERT, les s√©quences utilisent un d√©limiteur de d√©but de phrase `<s>` et de fin de phrase `</s>`:

```<s>Le chat est sur le matelas.</s>```

Pour les s√©quences trop courtes par rapport √† d'autres s√©quences du batch, le tokenizer rajoute un token `<pad>` qui ne sera pas consid√©r√© par le mod√®le.


```python
batch = next(iter(train_dataloader))
```


```python
print("\n".join(tokenizer.batch_decode(batch["input_ids"])))
batch["labels"]
```

    <s> H√© bien, c' est du bon travail tout √ßa</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Ok</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Qu'est-ce que je peux faire pour vous?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> C'est excellent!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> J'en voudrais bien</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Je peux vous aider?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Mais avant, voulez vous que je vous parle un peu de mon m√©tier, des √©tudes que j' ai faites, de mes coll√®gues, ou encore des normes de s√©curit√© applicables ici?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> non merci, nous allons y aller</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> d' abord, cherchez la machine qui convient sur laquelle monter votre moule. Avec la fiche du moule et celle des machines, √ßa ne devrait pas √™tre trop compliqu√©.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Bravo! Vous avez √©t√© rapides!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> C'est moi qui m'en occupe ici. C' est un m√©tier pour lequel il faut √™tre polyvalent et minutieux.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Je ne sais pas... mon oncle a dit que c' √©tait quelque part dans ce labo. Il faut fouiller. Regarde voir pr√®s du tableau noir l√† bas.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Ah, chouette, le design cela m'int√©resse!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> doucement man!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Alors, que voulez vous, dites moi?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Si vous avez besoins de trouver des plans, je peux vous aider. Mais je peux √©galement vous renseigner sur mon m√©tier, les √©tudes qu'il faut faire, le type de personne pour qui c'est appropri√©, ou encore les normes de s√©curit√©...</s>





    tensor([ 5,  0,  1,  0, 30,  1,  1, 18,  7,  5,  7,  7,  0, 19,  1,  7])



## Pr√©parer le mod√®le

Nous utilisons PyTorch Lightning, un utilitaire autour de PyTorch qui facilite l'entra√Ænement de mod√®les de Machine Learning notamment en supprimant les boucles d'entra√Ænement et d'optimisation √©crites √† la main.

Pour utiliser PL, nous allons "wrapper" notre mod√®le dans un `LightningModule` et impl√©menter trois m√©thodes essentielles :
- `training_step` prend en entr√©e les donn√©es d'un batch qui sont pass√©es au mod√®le et retourne la loss du mod√®le sur ce batch. C'est ici qu'on d√©finit la fonction de *CrossEntropy*.
- `validation_step` est similaire √† `training_step` mais retourne les m√©triques de validation utilis√©es (dans notre cas, l'exactitude ou *accuracy*).
- `configure_optimizers` retourne l'optimiseur que nous souhaitons utiliser pour l'entra√Ænement. L'un des optimiseurs les plus utilis√©s avec les mod√®les Transformers est AdamW, disponible directement dans `torch.optim`. On pr√©cise le taux d'apprentissage ou *learning rate* lors de la d√©finition de l'optimiseur.

Le *learning rate* permet d'ajuster la longueur du pas effectu√© √† chaque √©tape d'optimisation. Un pas trop long peut emp√™cher la convergence, mais un pas trop court peut allonger le temps d'entra√Ænement, comme le montre ce sch√©ma:
<!-- <p align="center">
  <img src="https://drive.google.com/uc?export=view&id=16K1eVDMc3FyA8Kx3u-o-pC6BaLAmWDPF" alt="learning rate"/>
</p> -->

![png](gradient_descent.png)


Pour rappel, la fonction cross-entropy est une mesure de divergence entre les pr√©dictions du mod√®le et les labels observ√©s. La cross-entropy est d√©finie par :

<!-- $$\text{CE}(y, \hat{y}) = \sum_i y_i \log(\hat{y}_i)$$ -->

$$ \text{softmax}(s) = \left( \frac{e^{s_i}}{\sum_k e^{s_k}} \right)_{i\in[|1,K|]} \text{for } s\in \mathbb{R}^K.$$

Texte et tout

<!-- <p align="center">
  <img src="https://drive.google.com/uc?id=1qtyaBx7EXgz7ATeQ4hnEFip-8EWJbb9l" alt="ce"/>
</p> -->



```python
class LightningModel(pl.LightningModule):
    def __init__(self, model_name, num_labels, lr, weight_decay, from_scratch=False):
        super().__init__()
        self.save_hyperparameters()
        if from_scratch:
            # Si `from_scratch` est vrai, on charge uniquement la config (nombre de couches, hidden size, etc.) et pas les poids du mod√®le 
            config = AutoConfig.from_pretrained(
                model_name, num_labels=num_labels
            )
            self.model = AutoModelForSequenceClassification.from_config(config)
        else:
            # Cette m√©thode permet de t√©l√©charger le bon mod√®le pr√©-entra√Æn√© directement depuis le Hub de HuggingFace sur lequel sont stock√©s de nombreux mod√®les
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_name, num_labels=num_labels
            )
        self.lr = lr
        self.weight_decay = weight_decay
        self.num_labels = self.model.num_labels

    def forward(self, batch):
        return self.model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"]
        )

    def training_step(self, batch):
        out = self.forward(batch)

        logits = out.logits
        # -------- MASKED --------
        loss_fn = torch.nn.CrossEntropyLoss()
        loss = loss_fn(logits.view(-1, self.num_labels), batch["labels"].view(-1))

        # ------ END MASKED ------

        self.log("train/loss", loss)

        return loss

    def validation_step(self, batch, batch_index):
        labels = batch["labels"]
        out = self.forward(batch)

        preds = torch.max(out.logits, -1).indices
        # -------- MASKED --------
        acc = (batch["labels"] == preds).float().mean()
        # ------ END MASKED ------
        self.log("valid/acc", acc)

        f1 = f1_score(batch["labels"].cpu().tolist(), preds.cpu().tolist(), average="macro")
        self.log("valid/f1", f1)

    def predict_step(self, batch, batch_idx):
        """La fonction predict step facilite la pr√©diction de donn√©es. Elle est 
        similaire √† `validation_step`, sans le calcul des m√©triques.
        """
        out = self.forward(batch)

        return torch.max(out.logits, -1).indices

    def configure_optimizers(self):
        return torch.optim.AdamW(
            self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay
        )
```


```python
lightning_model = LightningModel("camembert-base", num_labels, lr=3e-5, weight_decay=0.)
```

    Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']
    - This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


<!-- ### Suivre l'entra√Ænement

Pour suivre l'avancement de l'entra√Ænement du mod√®le, il est d'usage d'utiliser un "logger" (par exemple, TensorBoard ou Weights and Biases) qui permet d'afficher les graphes utiles : valeur de la loss, *accuracy* du mod√®le sur les donn√©es de validation, performances du GPU etc.  -->


## Lancer l'entra√Ænement

Avec PyTorch Lightning, pas besoin de boucle d'entra√Ænement √©crite √† la main comme avec PyTorch, le Trainer se charge de s√©lectionner les GPUs, s'arr√™ter au bon nombre d'epochs, et de nombreuses autres options. Si vous avez besoin d'une fonctionnalit√© sp√©ciale, il y a de grandes chances que celle-ci soit d√©j√† impl√©ment√©e dans Pytorch Lightning.

Cependant, l'entra√Ænement de mod√®les de Deep Learning demande de conna√Ætre quelques termes techniques, nous allons d√©crire ceux que vous pourrez croiser dans ce notebook ici :
- **epochs**: une passe sur toutes les donn√©es d'entra√Ænement. Si l'on fait 15 epochs, le mod√®le aura "vu" 15 fois les donn√©es.
- **early stopping**: technique qui consiste √† arr√™ter l'entra√Ænement du mod√®le lorsqu'une m√©trique (g√©n√©ralement la loss ou l'accuracy sur les donn√©es de validation) arr√™te de diminuer ou augmenter. Cela permet d'√©viter l'overfitting, c'est √† dire une m√©morisation des donn√©es d'entra√Ænement au d√©triment de la g√©n√©ralisation du mod√®le sur de nouvelles donn√©es. La **patience** sp√©cifie combien d'epochs attendre avant de stopper l'entra√Ænement si la m√©trique n'a toujours pas √©t√© am√©lior√©e.
- **model checkpoints**: des sauvegardes du mod√®le au fur et √† mesure de l'entra√Ænement. Ici, on demande √† PytorchLightning de sauver le meilleur mod√®le par rapport √† l'exactitude sur les donn√©es de validation.


```python
model_checkpoint = pl.callbacks.ModelCheckpoint(monitor="valid/acc", mode="max")

camembert_trainer = pl.Trainer(
    max_epochs=20,
    gpus=1,
    callbacks=[
        pl.callbacks.EarlyStopping(monitor="valid/acc", patience=4, mode="max"),
        model_checkpoint,
    ]
)
```

    GPU available: True, used: True
    TPU available: False, using: 0 TPU cores
    IPU available: False, using: 0 IPUs
    HPU available: False, using: 0 HPUs



```python
camembert_trainer.fit(lightning_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)
```

    Missing logger folder: /content/lightning_logs
    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
    
      | Name  | Type                               | Params
    -------------------------------------------------------------
    0 | model | CamembertForSequenceClassification | 110 M 
    -------------------------------------------------------------
    110 M     Trainable params
    0         Non-trainable params
    110 M     Total params
    442.583   Total estimated model params size (MB)


    Sanity Checking: 0it [00:00, ?it/s]

    /usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:73: UserWarning:
    
    Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
    
    Training: 0it [00:00, ?it/s]



R√©cup√©rons le meilleur mod√®le sauv√© par le *callback* `ModelCheckpoint`. Autrement, le mod√®le a toujours les derniers poids issus de l'optimisation, qui ne donnent pas forc√©ment la meilleure valeur de la m√©trique.


```python
lightning_model = LightningModel.load_from_checkpoint(checkpoint_path=model_checkpoint.best_model_path)
```

    Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']
    - This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


### Notebook checkpoint

Si jamais vous n'avez pas pu entra√Æner le mod√®le ci dessus, vous pouvez en t√©l√©charger une version similaire sauvegard√©e sur le Hub d'HuggingFace :




```python
lightning_model = LightningModel("RomanCast/camembert-miam-loria-finetuned", num_labels, lr=3e-5, weight_decay=0.)
```


## Inf√©rence avec le mod√®le entra√Æn√©


```python
ID_TO_LABEL = dataset["train"].features["Label"].names
```


```python
def get_preds(model, tokenizer, sentence):
    tokenized_sentence = tokenizer(sentence, return_tensors="pt")
    input_ids, attention_mask = tokenized_sentence.input_ids, tokenized_sentence.attention_mask

    out = model(
        input_ids=tokenized_sentence.input_ids,
        attention_mask=tokenized_sentence.attention_mask
    )

    logits = out.logits

    probas = torch.softmax(logits, -1).squeeze()

    pred = torch.argmax(probas)

    return ID_TO_LABEL[pred], probas[pred].item()
```

La fonction `get_preds` permet d'√©valuer simplement les sorties de notre mod√®le. Remplacez seulement la phrase dans `test_sentence` pour observer les sorties du mod√®le.


```python
test_sentence = "Bonjour, vous allez bien ?"

label_predicted, proba = get_preds(lightning_model.model, tokenizer, test_sentence)

print(f"Label: {label_predicted}, confidence: {proba:.2f}")
```

    Label: greet, confidence: 1.00


## Mod√®le pr√©-entra√Æn√© vs. mod√®le initialis√© al√©atoirement

Un moyen facile d'appr√©cier les gains du pr√©-entra√Ænement consiste √† entra√Æner un mod√®le dont les poids aurait √©t√© initialis√©s al√©atoirement. Alors qu'un mod√®le pr√©-entra√Æn√© a appris √† pr√©dire des gaps dans des millions de phrases, un mod√®le initialis√© al√©atoirement ne verra que les donn√©es de la t√¢che de *finetuning* qu'on lui donne.

Nous proposons donc ici d'utiliser un mod√®le n'ayant re√ßu aucun pr√©-entra√Ænement et de l'entra√Æner uniquement sur le dataset MIAM afin de quantifier exactement l'impact du pr√©-entra√Ænement.


```python
no_init_lightning_model = LightningModel("camembert-base", num_labels, lr=3e-5, weight_decay=0., from_scratch=True)
```


```python
model_checkpoint = pl.callbacks.ModelCheckpoint(monitor="valid/acc", mode="max")

no_init_trainer = pl.Trainer(
    max_epochs=30,
    gpus=1,
    callbacks=[
        pl.callbacks.EarlyStopping(monitor="valid/acc", patience=4, mode="max"),
        model_checkpoint,
    ]
)
```

    GPU available: True, used: True
    TPU available: False, using: 0 TPU cores
    IPU available: False, using: 0 IPUs
    HPU available: False, using: 0 HPUs



```python
no_init_trainer.fit(no_init_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)
```

    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
    
      | Name  | Type                               | Params
    -------------------------------------------------------------
    0 | model | CamembertForSequenceClassification | 110 M 
    -------------------------------------------------------------
    110 M     Trainable params
    0         Non-trainable params
    110 M     Total params
    442.583   Total estimated model params size (MB)


    Sanity Checking: 0it [00:00, ?it/s]

    Training: 0it [00:00, ?it/s]




```python
no_init_lightning_model = LightningModel.load_from_checkpoint(checkpoint_path=model_checkpoint.best_model_path)
```

Nous trouvons une accuracy de 83,65, quasiment 4 points de moins qu'avec le mod√®le pr√©-entra√Æn√© !

Comme pr√©c√©demment, si vous n'avez pas pu entra√Æn√© le mod√®le non-initialis√©, t√©l√©chargez le ici en d√©commentant cette cellule :


```python
lightning_model = LightningModel("RomanCast/no_init_miam_loria_finetuned", num_labels, lr=3e-5, weight_decay=0.)
```


### Matrice de confusion 

Nous allons regarder ici les matrices de confusion des deux mod√®les, c'est √† dire comparer les pr√©dictions des mod√®les aux labels originaux. Pour cela, nous faisons une pr√©diction compl√®te sur les donn√©es de validation √† l'aide du *trainer*.


```python
camembert_preds = camembert_trainer.predict(lightning_model, dataloaders=val_dataloader)
camembert_preds = torch.cat(camembert_preds, -1)
```

    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

    Predicting: 0it [00:00, ?it/s]



```python
no_init_preds = camembert_trainer.predict(no_init_lightning_model, dataloaders=val_dataloader)
no_init_preds = torch.cat(no_init_preds, -1)
```

    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

    Predicting: 0it [00:00, ?it/s]



```python
label_names = dataset["validation"].features["Label"].names
labels = dataset["validation"]["Label"]
```


```python
def plot_confusion_matrix(labels, preds, label_names):
    confusion_norm = confusion_matrix(labels, preds.tolist(), labels=list(range(len(label_names))), normalize="true")
    confusion = confusion_matrix(labels, preds.tolist(), labels=list(range(len(label_names))))
    
    plt.figure(figsize=(16, 14))
    sns.heatmap(
        confusion_norm,
        annot=confusion,
        cbar=False,
        fmt="d",
        xticklabels=label_names,
        yticklabels=label_names,
        cmap="viridis"
    )
```

Sur le graphe suivant, chaque ligne correspond au vrai label, chaque colonne au label pr√©dit. Par exemple, le mod√®le camembert a bien pr√©dit le label "ack" dans la plupart des cas, mais le confond r√©guli√®rement avec le label "yes". En revanche, le label "kindatt" est tout le temps confondu avec "ack".


```python
plot_confusion_matrix(labels, camembert_preds, label_names)
```


    
![png](output_124_0.png)
    



```python
plot_confusion_matrix(labels, no_init_preds, label_names)
```


    
![png](output_125_0.png)
    


<!-- ## Bonus : Comparaison avec un mod√®le multilingue : XLM-RoBERTa


```python
multilingual_tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
```

    Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight']
    - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.



```python
multilingual_lightning_model = LightningModel(
    "xlm-roberta-base", num_labels, lr=3e-5, weight_decay=0.
)

multiling_trainer = pl.Trainer(
    max_epochs=20,
    gpus=1,
    callbacks=[
        pl.callbacks.EarlyStopping(monitor="valid/acc", patience=4, mode="max"),
        pl.callbacks.ModelCheckpoint(monitor="valid/acc", mode="max")
    ]
)
```

    GPU available: True, used: True
    TPU available: False, using: 0 TPU cores
    IPU available: False, using: 0 IPUs
    HPU available: False, using: 0 HPUs


On remarque que le nombre de param√®tres de XLM-RoBERTa est beaucoup plus important.

En effet, puisque XLM-RoBERTa supporte plus d'une centaine de langues, son vocabulaire d'entr√©e est bien plus large. Or, chaque √©l√©ment du vocabulaire ayant un *embedding* correspondant.

Ainsi, le nombre de param√®tres contenus dans la couche d'embedding de CamemBERT est, en appelant $V$ la taille du vocabulaire et $H$ la *hidden size* du mod√®le :

$$V\ \times H = 32000 \times 768=25M$$

Dans XLM-RoBERTa on a :

$$V\ \times H = 250000 \times 768=192M$$

soit $167M$ de param√®tres en plus !


```python
multi_train_dataloader = DataLoader(
    dataset["train"], 
    batch_size=16, 
    shuffle=True, 
    collate_fn=functools.partial(tokenize_batch, tokenizer=multilingual_tokenizer)
)
multi_val_dataloader = DataLoader(
    dataset["validation"], 
    batch_size=16, 
    shuffle=False, 
    collate_fn=functools.partial(tokenize_batch, tokenizer=multilingual_tokenizer)
)
```


```python
multiling_trainer.fit(
    multilingual_lightning_model,
    train_dataloaders=multi_train_dataloader,
    val_dataloaders=multi_val_dataloader
)
```

    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
    
      | Name  | Type                                | Params
    --------------------------------------------------------------
    0 | model | XLMRobertaForSequenceClassification | 278 M 
    --------------------------------------------------------------
    278 M     Trainable params
    0         Non-trainable params
    278 M     Total params
    1,112.270 Total estimated model params size (MB)


    Sanity Checking: 0it [00:00, ?it/s]

    Training: 0it [00:00, ?it/s]

 -->
