---
title: "Hands-on CamemBERT, partie 2: Fine-tuner Camembert pour la Classification d'Acte de Dialogue"
date: "2022-07-06T00:00:01Z"
authors: 
- bmuller
- ngodey
- roman
categories: 
- nlp
- camembert
- classification
tags:

featured: true
---

<!-- Enable mathjax as well as inline math -->
{{< rawhtml >}}
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
{{< /rawhtml >}}

![png](/img/icon-192.png)

Ce tutoriel a √©t√© con√ßu dans le cadre des journ√©es [Deep Voice de l'IRCAM](https://www.ircam.fr/agenda/deep-voice-paris/detail) par Roman Castagn√©, Nathan Godey et Benjamin Muller.

Une version du tutoriel en anglais en version pdf slide est disponible [ici](/pdf/hands-on-camembert-tutorial-slides-en.pdf), ainsi que le m√™me tutoriel dans [Google Colab](https://colab.research.google.com/drive/1W0Fj7aXm2qPx34PbEo0F5s5_sm8vVrJl?usp=sharing).

Suite √† la [premi√®re partie]({{< relref "posts/tutorial" >}}) qui introduit le fonctionnement de CamemBERT en tant que mod√®le de langue, nous allons d√©sormais √©tudier comment entrainer et √©valuer (*fine-tuner* pour √™tre pr√©cis!) CamemBERT pour notre t√¢che de classification d'acte de dialogue.

&nbsp;

# Dans ce Tutoriel:

Ce tutoriel est une introduction au Natural Language Processing et en particulier au mod√®le de langue de type BERT. 

## Pour cela

Nous allons nous int√©resser √† une t√¢che de classification de s√©quence: **la t√¢che de pr√©diction d'acte de dialogue**. 

Nous allons utiliser le dataset MIAM (introduit [dans cet article](https://aclanthology.org/2021.emnlp-main.656.pdf)) afin d'entra√Æner et d'√©valuer nos mod√®les sur cette t√¢che. 

Nous travaillerons avec la librairie [Transformers](https://huggingface.co/docs/transformers/index) de ü§ó Hugging-Face ainsi que la librairie [Pytorch Lightning](https://www.pytorchlightning.ai).

## Pr√©requis

- bases en python 
- bases en machine learning

&nbsp;

# Partie 2 : *Finetuning* pour la classification de s√©quences

Dans ce chapitre, nous allons finetuner CamemBERT pour une t√¢che de classification de s√©quences.

L'objectif de cette partie est double :
- Apprendre √† utiliser des modules Python modernes qui permettent d'entra√Æner rapidement les mod√®les de langue (HuggingFace Transformers et Datasets, Pytorch Lightning) ;
- Etudier les performances du mod√®le de fran√ßais CamemBERT notamment en comparaison avec un mod√®le sans pr√©-entra√Ænement pr√©alable (uniquement entra√Æn√© sur la t√¢che de classification) et un mod√®le avec un pr√©-entra√Ænement multilingue.



### Inspection des donn√©es pour le mod√®le

Notre jeu de donn√©es √©tant relativement petit, nous faisons le choix de tokeniser nos donn√©es juste avant de les fournir au mod√®le. La pr√©paration est relativement simple dans notre cas. Nous allons devoir tokeniser les donn√©es, les "padder" (i.e. s'assurer que toutes les s√©quences ont la m√™me longueur) et rajouter des tokens sp√©ciaux. 

Toutes ces op√©rations ont d√©j√† √©t√© faites dans la premi√®re partie du notebook, nous r√©utilisons donc le dataset et la *collate function* `tokenize_batch` afin d'instancier un `DataLoader` qui va fournir les donn√©es au mod√®le en *batch*.

Un batch est un groupe d'exemples donn√©s au mod√®le √† une √©tape d'entra√Ænement. La *loss* et les gradients sont calcul√©s et moyenn√©s pour l'ensemble du batch. Cela permet:
- d'√©viter une trop grande variance des gradients utilis√©s pour la descente de gradient stochastique: en moyennant les gradients, la direction de descente estim√©e √† chaque pas est plus proche de la direction de descente th√©orique;
- de profiter de la parall√©lisation des calculs offerte par les GPUs pour certaines op√©rations. On peut ainsi observer une inf√©rence plus rapide avec de plus grands batchs pour un m√™me nombre d'exemples total.


```python
num_labels = dataset["train"].features["Label"].num_classes
```

Re-regardons quelques exemples de phrases issues du jeu de donn√©es MIAM. On retrouve √† gauche les labels que nous allons devoir pr√©dire.


```python
pd_dataset["validation"][["Dialogue_Act", "Utterance"]].head(20)
```

{{< rawhtml >}}
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Dialogue_Act</th>
      <th>Utterance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>greet</td>
      <td>Bravo! Vous avez √©t√© rapides!</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ask</td>
      <td>Qu'est-ce que je peux faire pour vous?</td>
    </tr>
    <tr>
      <th>2</th>
      <td>next_step</td>
      <td>merci</td>
    </tr>
    <tr>
      <th>3</th>
      <td>inform</td>
      <td>Eh bien, il va falloir la fabriquer cette manette. Allez voir M√©lissa, la responsable d' √Ælot de production, elle va vous aider. Elle doit √™tre dans l' atelier de fabrication.</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ask</td>
      <td>Mais sinon, vous avez encore des questions sur l'entreprise ? Le staff de l'entreprise, ou bien les √©tudes qu'ils ont fait ?</td>
    </tr>
    <tr>
      <th>5</th>
      <td>staff_enterprise</td>
      <td>j'ai un peu de temps... allez y dites moi en plus sur l'entreprise, le staff et sur les √©tudes de chacun...</td>
    </tr>
    <tr>
      <th>6</th>
      <td>inform</td>
      <td>la plasturgie, c' est plus une question de logique que de force physique. que ce soit pour imaginer les produits de demain comme ici, au bureau d' √©tudes, ou pour les cr√©er, comme dans l' atelier, tout le monde y trouve son compte, filles comme gar√ßons Il y a tellement de m√©tiers diff√©rents et int√©ressants.</td>
    </tr>
    <tr>
      <th>7</th>
      <td>ask</td>
      <td>et vous vouliez savoir quelles √©tudes ils avaient fait c'est √ßa ?</td>
    </tr>
    <tr>
      <th>8</th>
      <td>yes</td>
      <td>oui oui</td>
    </tr>
    <tr>
      <th>9</th>
      <td>inform</td>
      <td>Il y a des m√©tiers pour tout le monde¬†du BEP √† l' ing√©nieur. mais en fait, peu importe le niveau d' √©tudes¬†il y a toujours des possibilit√©s d' √©volution.</td>
    </tr>
    <tr>
      <th>10</th>
      <td>quit</td>
      <td>Bon, je dois vous laisser. Bon courage pour la suite !</td>
    </tr>
    <tr>
      <th>11</th>
      <td>greet</td>
      <td>Bravo! Vous avez √©t√© rapides!</td>
    </tr>
    <tr>
      <th>12</th>
      <td>ask</td>
      <td>Qu'est-ce que je peux faire pour vous?</td>
    </tr>
    <tr>
      <th>13</th>
      <td>next_step</td>
      <td>est ce que je peux utiliser la manette maintenant ?</td>
    </tr>
    <tr>
      <th>14</th>
      <td>inform</td>
      <td>Il ne vous reste plus qu' √† trouver les composants √©lectroniques pour terminer cette manette. Sophia doit les avoir sur elle, je l' ai vue passer tout √† l' heure.</td>
    </tr>
    <tr>
      <th>15</th>
      <td>ask</td>
      <td>Mais avant de partir, voulez vous faire un mini quizz avec moi ?</td>
    </tr>
    <tr>
      <th>16</th>
      <td>yes</td>
      <td>oui</td>
    </tr>
    <tr>
      <th>17</th>
      <td>ask</td>
      <td>Que faudra-t-il faire de votre manette une fois qu'elle sera cass√©e et irr√©aparable ?</td>
    </tr>
    <tr>
      <th>18</th>
      <td>todo_irreparable</td>
      <td>la recycler ?</td>
    </tr>
    <tr>
      <th>19</th>
      <td>inform</td>
      <td>tout √† fait, on ram√®ne les √©quipements hors-service au magasin, ils pourront les faire recycler</td>
    </tr>
  </tbody>
</table>
{{< /rawhtml >}}

Il est important, en particulier avec les petits jeux de donn√©es, que le mod√®le ne voit pas les donn√©es dans le m√™me ordre au fur et √† mesure des epochs. Pour √©viter cela, on utilise l'argument `shuffle` du DataLoader.


```python
train_dataloader = DataLoader(
    dataset["train"], 
    batch_size=16, 
    shuffle=True, 
    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer)
)
val_dataloader = DataLoader(
    dataset["validation"], 
    batch_size=16, 
    shuffle=False, 
    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer)
)
```

Une bonne mani√®re de s'assurer que les donn√©es sur lesquelles le mod√®le va s'entra√Æner sont dans un format correct est de regarder un batch et d√©coder les indices issus du tokenizer. On retrouve des tokens ajout√©s : les tokens sp√©ciaux et le padding pour compenser les phrases trop courtes.

On remarque notamment que dans CamemBERT, les s√©quences utilisent un d√©limiteur de d√©but de phrase `<s>` et de fin de phrase `</s>`:

```<s>Le chat est sur le matelas.</s><pad><pad>```

Pour les s√©quences trop courtes par rapport √† d'autres s√©quences du batch, le tokenizer rajoute un token `<pad>` qui ne sera pas consid√©r√© par le mod√®le.


```python
batch = next(iter(train_dataloader))
```


```python
print("\n".join(tokenizer.batch_decode(batch["input_ids"])))
batch["labels"]
```

    <s> H√© bien, c' est du bon travail tout √ßa</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Ok</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Qu'est-ce que je peux faire pour vous?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> C'est excellent!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> J'en voudrais bien</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Je peux vous aider?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Mais avant, voulez vous que je vous parle un peu de mon m√©tier, des √©tudes que j' ai faites, de mes coll√®gues, ou encore des normes de s√©curit√© applicables ici?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> non merci, nous allons y aller</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> d' abord, cherchez la machine qui convient sur laquelle monter votre moule. Avec la fiche du moule et celle des machines, √ßa ne devrait pas √™tre trop compliqu√©.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Bravo! Vous avez √©t√© rapides!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> C'est moi qui m'en occupe ici. C' est un m√©tier pour lequel il faut √™tre polyvalent et minutieux.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Je ne sais pas... mon oncle a dit que c' √©tait quelque part dans ce labo. Il faut fouiller. Regarde voir pr√®s du tableau noir l√† bas.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Ah, chouette, le design cela m'int√©resse!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> doucement man!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Alors, que voulez vous, dites moi?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
    <s> Si vous avez besoins de trouver des plans, je peux vous aider. Mais je peux √©galement vous renseigner sur mon m√©tier, les √©tudes qu'il faut faire, le type de personne pour qui c'est appropri√©, ou encore les normes de s√©curit√©...</s>





    tensor([ 5,  0,  1,  0, 30,  1,  1, 18,  7,  5,  7,  7,  0, 19,  1,  7])

&nbsp;

## Pr√©parer le mod√®le

Nous utilisons PyTorch Lightning, un utilitaire autour de PyTorch qui facilite l'entra√Ænement de mod√®les de Machine Learning notamment en supprimant les boucles d'entra√Ænement et d'optimisation √©crites √† la main.

Pour utiliser PL, nous allons "wrapper" notre mod√®le dans un `LightningModule` et impl√©menter trois m√©thodes essentielles :
- `training_step` prend en entr√©e les donn√©es d'un batch qui sont pass√©es au mod√®le et retourne la loss du mod√®le sur ce batch. C'est ici qu'on d√©finit la fonction de *CrossEntropy*;
- `validation_step` est similaire √† `training_step` mais retourne les m√©triques de validation utilis√©es (dans notre cas, l'exactitude ou *accuracy*);
- `configure_optimizers` retourne l'optimiseur que nous souhaitons utiliser pour l'entra√Ænement. L'un des optimiseurs les plus utilis√©s avec les mod√®les Transformers est AdamW, disponible directement dans `torch.optim`. On pr√©cise le taux d'apprentissage ou *learning rate* lors de la d√©finition de l'optimiseur.

Le *learning rate* permet d'ajuster la longueur du pas effectu√© √† chaque √©tape d'optimisation. Un pas trop long peut emp√™cher la convergence, mais un pas trop court peut allonger le temps d'entra√Ænement, comme le montre ce sch√©ma:
<!-- <p align="center">
  <img src="https://drive.google.com/uc?export=view&id=16K1eVDMc3FyA8Kx3u-o-pC6BaLAmWDPF" alt="learning rate"/>
</p> -->

![png](gradient_descent.png)


Pour rappel, la fonction d'entropie crois√©e (ou *cross entropy* en anglais) est une mesure de divergence entre les pr√©dictions du mod√®le et les labels observ√©s. Soit $y$ les vrais labels et $\hat{y}$ les scores donn√©s par le mod√®le aux vrais labels. L'entropie crois√©e est d√©finie par :

$$\text{CE}(y, \hat{y}) = -\sum_i y_i \log(\hat{y}_i)$$



<!-- <p align="center">
  <img src="https://drive.google.com/uc?id=1qtyaBx7EXgz7ATeQ4hnEFip-8EWJbb9l" alt="ce"/>
</p> -->



```python
class LightningModel(pl.LightningModule):
    def __init__(self, model_name, num_labels, lr, weight_decay, from_scratch=False):
        super().__init__()
        self.save_hyperparameters()
        if from_scratch:
            # Si `from_scratch` est vrai, on charge uniquement la config (nombre de couches, hidden size, etc.) et pas les poids du mod√®le 
            config = AutoConfig.from_pretrained(
                model_name, num_labels=num_labels
            )
            self.model = AutoModelForSequenceClassification.from_config(config)
        else:
            # Cette m√©thode permet de t√©l√©charger le bon mod√®le pr√©-entra√Æn√© directement depuis le Hub de HuggingFace sur lequel sont stock√©s de nombreux mod√®les
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_name, num_labels=num_labels
            )
        self.lr = lr
        self.weight_decay = weight_decay
        self.num_labels = self.model.num_labels

    def forward(self, batch):
        return self.model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"]
        )

    def training_step(self, batch):
        out = self.forward(batch)

        logits = out.logits
        # -------- MASKED --------
        loss_fn = torch.nn.CrossEntropyLoss()
        loss = loss_fn(logits.view(-1, self.num_labels), batch["labels"].view(-1))

        # ------ END MASKED ------

        self.log("train/loss", loss)

        return loss

    def validation_step(self, batch, batch_index):
        labels = batch["labels"]
        out = self.forward(batch)

        preds = torch.max(out.logits, -1).indices
        # -------- MASKED --------
        acc = (batch["labels"] == preds).float().mean()
        # ------ END MASKED ------
        self.log("valid/acc", acc)

        f1 = f1_score(batch["labels"].cpu().tolist(), preds.cpu().tolist(), average="macro")
        self.log("valid/f1", f1)

    def predict_step(self, batch, batch_idx):
        """La fonction predict step facilite la pr√©diction de donn√©es. Elle est 
        similaire √† `validation_step`, sans le calcul des m√©triques.
        """
        out = self.forward(batch)

        return torch.max(out.logits, -1).indices

    def configure_optimizers(self):
        return torch.optim.AdamW(
            self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay
        )
```


```python
lightning_model = LightningModel("camembert-base", num_labels, lr=3e-5, weight_decay=0.)
```

    Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']
    - This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


<!-- ### Suivre l'entra√Ænement

Pour suivre l'avancement de l'entra√Ænement du mod√®le, il est d'usage d'utiliser un "logger" (par exemple, TensorBoard ou Weights and Biases) qui permet d'afficher les graphes utiles : valeur de la loss, *accuracy* du mod√®le sur les donn√©es de validation, performances du GPU etc.  -->


## Lancer l'entra√Ænement

Avec PyTorch Lightning, plus besoin de boucle d'entra√Ænement √©crite √† la main comme avec PyTorch. 

En effet, le Trainer se charge de s√©lectionner les GPUs, s'arr√™ter au bon nombre d'epochs, et de nombreuses autres options. Si vous avez besoin d'une fonctionnalit√© sp√©ciale, il y a de grandes chances que celle-ci soit d√©j√† impl√©ment√©e dans Pytorch Lightning.

Cependant, l'entra√Ænement de mod√®les de Deep Learning demande de conna√Ætre quelques termes techniques, nous allons d√©crire ceux que vous pourrez croiser dans ce notebook ici :
- **epochs :** une passe sur toutes les donn√©es d'entra√Ænement. Si l'on fait 15 epochs, le mod√®le aura "vu" 15 fois les donn√©es ;
- **early stopping :** technique qui consiste √† arr√™ter l'entra√Ænement du mod√®le lorsqu'une m√©trique (g√©n√©ralement la loss ou l'accuracy sur les donn√©es de validation) arr√™te de diminuer ou augmenter. Cela permet d'√©viter l'overfitting, c'est √† dire une m√©morisation des donn√©es d'entra√Ænement au d√©triment de la g√©n√©ralisation du mod√®le sur de nouvelles donn√©es. La **patience** sp√©cifie combien d'epochs attendre avant de stopper l'entra√Ænement si la m√©trique n'a toujours pas √©t√© am√©lior√©e ;
- **model checkpoints :** des sauvegardes du mod√®le au fur et √† mesure de l'entra√Ænement. Ici, on demande √† PytorchLightning de sauver le meilleur mod√®le par rapport √† l'exactitude sur les donn√©es de validation. Ces *checkpoints* sont ensuite utilis√©s pour relancer l'entra√Ænement en cas d'arr√™t pr√©matur√©, ou pour sauvegarder la meilleure version de notre mod√®le.


```python
model_checkpoint = pl.callbacks.ModelCheckpoint(monitor="valid/acc", mode="max")

camembert_trainer = pl.Trainer(
    max_epochs=20,
    gpus=1,
    callbacks=[
        pl.callbacks.EarlyStopping(monitor="valid/acc", patience=4, mode="max"),
        model_checkpoint,
    ]
)
```

    GPU available: True, used: True
    TPU available: False, using: 0 TPU cores
    IPU available: False, using: 0 IPUs
    HPU available: False, using: 0 HPUs



```python
camembert_trainer.fit(lightning_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)
```

    Missing logger folder: /content/lightning_logs
    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
    
      | Name  | Type                               | Params
    -------------------------------------------------------------
    0 | model | CamembertForSequenceClassification | 110 M 
    -------------------------------------------------------------
    110 M     Trainable params
    0         Non-trainable params
    110 M     Total params
    442.583   Total estimated model params size (MB)


    Sanity Checking: 0it [00:00, ?it/s]

    /usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:73: UserWarning:
    
    Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
    
    Training: 0it [00:00, ?it/s]



R√©cup√©rons le meilleur mod√®le sauv√© par l'utilitaire `ModelCheckpoint` de Pytorch Lightning. Autrement, le mod√®le poss√®de toujours les derniers param√®tres issus de l'optimisation, qui ne correspondent pas forc√©ment √† la meilleure valeur de la m√©trique.


```python
lightning_model = LightningModel.load_from_checkpoint(checkpoint_path=model_checkpoint.best_model_path)
```

    Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']
    - This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


### üöß Notebook checkpoint

Si jamais vous n'avez pas pu entra√Æner le mod√®le ci dessus, vous pouvez en t√©l√©charger une version similaire sauvegard√©e sur le Hub d'HuggingFace :




```python
lightning_model = LightningModel("RomanCast/camembert-miam-loria-finetuned", num_labels, lr=3e-5, weight_decay=0.)
```

&nbsp;

## Inf√©rence avec le mod√®le entra√Æn√©


```python
ID_TO_LABEL = dataset["train"].features["Label"].names
```


```python
def get_preds(model, tokenizer, sentence):
    tokenized_sentence = tokenizer(sentence, return_tensors="pt")
    input_ids, attention_mask = tokenized_sentence.input_ids, tokenized_sentence.attention_mask

    out = model(
        input_ids=tokenized_sentence.input_ids,
        attention_mask=tokenized_sentence.attention_mask
    )

    logits = out.logits

    probas = torch.softmax(logits, -1).squeeze()

    pred = torch.argmax(probas)

    return ID_TO_LABEL[pred], probas[pred].item()
```

La fonction `get_preds` permet d'√©valuer simplement les sorties de notre mod√®le. Remplacez seulement la phrase dans `test_sentence` pour observer les sorties du mod√®le.


```python
test_sentence = "Bonjour, vous allez bien ?"

label_predicted, proba = get_preds(lightning_model.model, tokenizer, test_sentence)

print(f"Label: {label_predicted}, confidence: {proba:.2f}")
```

    Label: greet, confidence: 1.00


## Mod√®le pr√©-entra√Æn√© vs. mod√®le initialis√© al√©atoirement

Un moyen facile d'appr√©cier les gains du pr√©-entra√Ænement consiste √† entra√Æner un mod√®le dont les poids aurait √©t√© initialis√©s al√©atoirement. Alors qu'un mod√®le pr√©-entra√Æn√© a appris √† pr√©dire des gaps dans des millions de phrases, un mod√®le initialis√© al√©atoirement ne verra que les donn√©es de la t√¢che de *finetuning* qu'on lui donne.

Nous proposons donc ici d'utiliser un mod√®le n'ayant re√ßu aucun pr√©-entra√Ænement et de l'entra√Æner uniquement sur le dataset MIAM afin de quantifier exactement l'impact du pr√©-entra√Ænement.


```python
no_init_lightning_model = LightningModel("camembert-base", num_labels, lr=3e-5, weight_decay=0., from_scratch=True)
```


```python
model_checkpoint = pl.callbacks.ModelCheckpoint(monitor="valid/acc", mode="max")

no_init_trainer = pl.Trainer(
    max_epochs=30,
    gpus=1,
    callbacks=[
        pl.callbacks.EarlyStopping(monitor="valid/acc", patience=4, mode="max"),
        model_checkpoint,
    ]
)
```

    GPU available: True, used: True
    TPU available: False, using: 0 TPU cores
    IPU available: False, using: 0 IPUs
    HPU available: False, using: 0 HPUs



```python
no_init_trainer.fit(no_init_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)
```

    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
    
      | Name  | Type                               | Params
    -------------------------------------------------------------
    0 | model | CamembertForSequenceClassification | 110 M 
    -------------------------------------------------------------
    110 M     Trainable params
    0         Non-trainable params
    110 M     Total params
    442.583   Total estimated model params size (MB)


    Sanity Checking: 0it [00:00, ?it/s]

    Training: 0it [00:00, ?it/s]




```python
no_init_lightning_model = LightningModel.load_from_checkpoint(checkpoint_path=model_checkpoint.best_model_path)
```

Nous trouvons une accuracy de 83,65, quasiment 4 points de moins qu'avec le mod√®le pr√©-entra√Æn√© !

Comme pr√©c√©demment, si vous n'avez pas pu entra√Æn√© le mod√®le non-initialis√©, t√©l√©chargez le ici en d√©commentant cette cellule :


```python
lightning_model = LightningModel("RomanCast/no_init_miam_loria_finetuned", num_labels, lr=3e-5, weight_decay=0.)
```


### Matrice de confusion 

Nous allons regarder ici les matrices de confusion des deux mod√®les, c'est √† dire comparer les pr√©dictions des mod√®les aux labels originaux. Pour cela, nous faisons une pr√©diction compl√®te sur les donn√©es de validation √† l'aide du *trainer*.


```python
camembert_preds = camembert_trainer.predict(lightning_model, dataloaders=val_dataloader)
camembert_preds = torch.cat(camembert_preds, -1)
```

    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

    Predicting: 0it [00:00, ?it/s]



```python
no_init_preds = camembert_trainer.predict(no_init_lightning_model, dataloaders=val_dataloader)
no_init_preds = torch.cat(no_init_preds, -1)
```

    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

    Predicting: 0it [00:00, ?it/s]



```python
label_names = dataset["validation"].features["Label"].names
labels = dataset["validation"]["Label"]
```


```python
def plot_confusion_matrix(labels, preds, label_names):
    confusion_norm = confusion_matrix(labels, preds.tolist(), labels=list(range(len(label_names))), normalize="true")
    confusion = confusion_matrix(labels, preds.tolist(), labels=list(range(len(label_names))))
    
    plt.figure(figsize=(16, 14))
    sns.heatmap(
        confusion_norm,
        annot=confusion,
        cbar=False,
        fmt="d",
        xticklabels=label_names,
        yticklabels=label_names,
        cmap="viridis"
    )
```

Sur le graphe suivant, chaque ligne correspond au vrai label, chaque colonne au label pr√©dit. Par exemple, le mod√®le camembert a bien pr√©dit le label "ack" dans la plupart des cas, mais le confond r√©guli√®rement avec le label "yes". En revanche, le label "kindatt" est tout le temps confondu avec "ack" - il est probablement trop peu pr√©sent dans les donn√©es d'entra√Ænement pour √™tre vraiment "appris" par le mod√®le.

Ces matrices de confusion sont un outil pr√©cieux : elles permettent de mieux estimer o√π se situent les erreurs du mod√®le, pour ensuite pouvoir les corriger.


```python
plot_confusion_matrix(labels, camembert_preds, label_names)
```


    
![png](output_124_0.png)
    



```python
plot_confusion_matrix(labels, no_init_preds, label_names)
```


    
![png](output_125_0.png)
    


<!-- ## Bonus : Comparaison avec un mod√®le multilingue : XLM-RoBERTa


```python
multilingual_tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
```

    Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight']
    - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.



```python
multilingual_lightning_model = LightningModel(
    "xlm-roberta-base", num_labels, lr=3e-5, weight_decay=0.
)

multiling_trainer = pl.Trainer(
    max_epochs=20,
    gpus=1,
    callbacks=[
        pl.callbacks.EarlyStopping(monitor="valid/acc", patience=4, mode="max"),
        pl.callbacks.ModelCheckpoint(monitor="valid/acc", mode="max")
    ]
)
```

    GPU available: True, used: True
    TPU available: False, using: 0 TPU cores
    IPU available: False, using: 0 IPUs
    HPU available: False, using: 0 HPUs


On remarque que le nombre de param√®tres de XLM-RoBERTa est beaucoup plus important.

En effet, puisque XLM-RoBERTa supporte plus d'une centaine de langues, son vocabulaire d'entr√©e est bien plus large. Or, chaque √©l√©ment du vocabulaire ayant un *embedding* correspondant.

Ainsi, le nombre de param√®tres contenus dans la couche d'embedding de CamemBERT est, en appelant $V$ la taille du vocabulaire et $H$ la *hidden size* du mod√®le :

$$V\ \times H = 32000 \times 768=25M$$

Dans XLM-RoBERTa on a :

$$V\ \times H = 250000 \times 768=192M$$

soit $167M$ de param√®tres en plus !


```python
multi_train_dataloader = DataLoader(
    dataset["train"], 
    batch_size=16, 
    shuffle=True, 
    collate_fn=functools.partial(tokenize_batch, tokenizer=multilingual_tokenizer)
)
multi_val_dataloader = DataLoader(
    dataset["validation"], 
    batch_size=16, 
    shuffle=False, 
    collate_fn=functools.partial(tokenize_batch, tokenizer=multilingual_tokenizer)
)
```


```python
multiling_trainer.fit(
    multilingual_lightning_model,
    train_dataloaders=multi_train_dataloader,
    val_dataloaders=multi_val_dataloader
)
```

    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
    
      | Name  | Type                                | Params
    --------------------------------------------------------------
    0 | model | XLMRobertaForSequenceClassification | 278 M 
    --------------------------------------------------------------
    278 M     Trainable params
    0         Non-trainable params
    278 M     Total params
    1,112.270 Total estimated model params size (MB)


    Sanity Checking: 0it [00:00, ?it/s]

    Training: 0it [00:00, ?it/s]

 -->
